{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "9NEbB0cULdNj"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "import os, re\n",
    "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
    "    !pip install unsloth\n",
    "else:\n",
    "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
    "    import torch; v = re.match(r\"[0-9\\.]{3,}\", str(torch.__version__)).group(0)\n",
    "    xformers = \"xformers==\" + (\"0.0.32.post2\" if v == \"2.8.0\" else \"0.0.29.post3\")\n",
    "    !pip install --no-deps bitsandbytes accelerate {xformers} peft trl triton cut_cross_entropy unsloth_zoo\n",
    "    !pip install sentencepiece protobuf \"datasets>=3.4.1,<4.0.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n",
    "    !pip install --no-deps unsloth\n",
    "!pip install transformers==4.55.4\n",
    "!pip install --no-deps trl==0.22.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 476,
     "referenced_widgets": [
      "5fcdfe4b92c145beb8e1136cebf636a0",
      "21e9e3b336344ddea6fe80c8ad0bfbc0",
      "0c9d8783f5ac41dc82e7810566a239d3",
      "476ff82cc5784e999855a76761f2bd4f",
      "b816b931b020450ab5ee5a1930674602",
      "62c479e283384c2bad6ff2ff153431bb",
      "85ed5117b4ec495dae543bbecc33f37e",
      "fff85f027968421ea2b9268d67129aec",
      "14b8a641d56549aaaa8e6092b8250276",
      "72c8ae9b864047f5ae6e959f938824d1",
      "30c9234230a74c16acf4df08647a495a",
      "d59d3eccfe424336b58a11dc45658d2e",
      "715bee3b8d4549e597feb94e94a67b3a",
      "babe9aaac102483999b669fe61f83c31",
      "907959314dbc4000b6b077c251ed05c7",
      "15cbdbf40182466bb36a274bb5456f5c",
      "f6027df3e8d148b9b68982a9338c9b11",
      "859267e3fd5f40d6989905518b175664",
      "65a99310d4aa4d8c9b302c66fea95c79",
      "69ea9302790940ca8c1b0dd133bce375",
      "a4bcee19b201440ba653e2c4e92ae063",
      "bed89dc83e8442c4ba8fcf2ccebe9770",
      "a616543894e540b2a02ce8acdb34787e",
      "801737fab8e84f50bdb0e915562c1a57",
      "36338157ebf1457ebfb617a8ed46aa27",
      "a8af636e4fae4ff8824a66b2468a4230",
      "70458da27c824ce3a66d90f08dd45e91",
      "3520de9bd06f4e1987e7b0928535b169",
      "8a42878b433a406296bdb243fd3b6955",
      "fd9d48831d7d4e8aa6d44d30b8d39fac",
      "bdbb80606dc741feb98f542167a08d42",
      "f039d4c0622c4898b8e08719c9975591",
      "d29700a5e9ea42f2997fa0647acbe10d",
      "ac4cdde95e324b2ea6ab92c380279039",
      "5b0cdd1755a44bb1b3a604c604a19ad4",
      "f43a292adde1440f805233fec8a975a1",
      "77525e3136a54f69b5f608f6ea30ef64",
      "0d34083a143e4295b532a0a0fbd5fce3",
      "5b568c4a083e41f3b87c3769b3d64101",
      "0362e3dd51884ba3b78cfc7fa8ea23ae",
      "6fdd3af9f1a943238590ad4ddda0cf49",
      "c06ade4e4d7f47c0a0209652ac6cddd5",
      "319632b390364491bfc3704f08ed57c1",
      "cbd03c313bcf409fb55a04ce27815d30",
      "64509c3931c84370960e2f8176e57df1",
      "0e3af8510a2d4ce5b86eb4b6ef101b79",
      "03921e8776a4432d89306354d6cd6469",
      "0a0672d01cf4479f8d68f63ecbd14542",
      "8b31bb625c6c4b2997968ba4c0aa3adc",
      "e1e6d8df0ab747008a0bf242bd6cc923",
      "6c3be3a26d534b55a99293ded9d4c1a5",
      "2f4774af1c8240a6b7d5243e99df8e00",
      "a8d9d59842e547e39e8ad8b9c84fafec",
      "c8c4afdae2d843c09f9a52ebce160d10",
      "3b8905b7964441818345d81b839f8db1",
      "7ff109ce990243beb0f88407e6d2c446",
      "05cca4b0a78748bdbee050e9fea467d5",
      "f0e7ff96c704489bb72f29b14a7e4d78",
      "ce8fed05b2594302897eb15f9d94e5fc",
      "d4cb85cb873d48ddbf3692298884e6a9",
      "ce132bc2a80e4cd38da9cae326193e6c",
      "1aad3f426e0e43f59e03da1e22944493",
      "816ed93f31434528af4168c79d40d85d",
      "bbdbb4ee6cc74be1964ac62ad073bb9f",
      "aa01163cb9de4137baf530dd1dd47caa",
      "832bc5df5d4b44a0bdef179b2832ae72",
      "beea42fad99e499b851b831f9db42c0c",
      "67fb0868983f4aaba946d6363019e015",
      "162236cd1e48471bbd3ef149fc818ff0",
      "9591306182884544be2eb0078329270e",
      "587c4cfe6ad6490e8cbe6f677873c197",
      "61d92e34f47049268a9734db334c97e5",
      "304bd8b01f914060ad506dbaa0a01762",
      "55f39686b9e24a7cac104903f41f5020",
      "e9d609cf09ae4813a4b3426082bee727",
      "3d99bee68fb34def9b9e254bb2f46c29",
      "4bad3ba802c74830b22dfc9a9d351e09",
      "a1bfae8a68ac42f2857bfcee4e77186f",
      "0f057896eee240d1aacef6c8a9337867",
      "20f4c4949ca7457f8c5dab6ed23de0aa",
      "4384509aec944ea88cb670a33fe1370c",
      "27f23fcdd4b04d9c865847661a70414e",
      "169398aeb23d44ca8f1259775960e4f2",
      "1b96c2b528ef4675935689a3a81ca1fd",
      "8601361630404d3ea2a52c622dce63b5",
      "8fdb000428284995b940681b781e0b27",
      "0ab4750a5e824dd9adf5e93a9e784d02",
      "a37d82d6be0b41a7aaa8e560066fd355",
      "fef85aa562c7460ab1796a41dbaf31da",
      "da320bbf23bd4d998036c40662e8a258",
      "247580db82a94291ad620f87e2703853",
      "80ff840311b543f1a733f2e0994b7936",
      "9bcd611e5edb4ffbb7ee5ea6014a94fe",
      "31385abe2a5f4e65aaed80d2bd1efe8f",
      "788b84bb5da2429da64be3088f7599e0",
      "855b2a07426e47c28e6e2584f7e55954",
      "e70fb1b3b80344529e68d694b7f46d3e",
      "dbc7564e55ae4fe7bf7dc248c01695d1",
      "c7026ee1d7774927b31f2b1be1e88f36",
      "659c3bf33eb74a84ae18c5985efd0fc1",
      "0dc8cf552a2145cfa5d3a137d1a8196f",
      "186a2a0f9c58410ba0174382b1e3138c",
      "8c64981a8ec940a48813da66c91436af",
      "46630ac5ea214fb5832c87a31553947e",
      "c32edaa58c8e47efb1d6823cad1b5fa2",
      "5b208b2cdbb548ad83b17d72e2df3197",
      "bb3299ffbb724e90a1f5d7237a535496",
      "71a57f2760c24eeda7837501f29c07b3",
      "096ce056e0994da9ad97549897c2ebfd",
      "7e4f996bba13465eba16fb6e9c865d9e"
     ]
    },
    "id": "cELdKF9EL64F",
    "outputId": "ea98c48f-a7b8-4b01-fdaa-bb36d71b913e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "==((====))==  Unsloth 2025.9.9: Fast Llama patching. Transformers: 4.55.4.\n",
      "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.32.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fcdfe4b92c145beb8e1136cebf636a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d59d3eccfe424336b58a11dc45658d2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a616543894e540b2a02ce8acdb34787e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/1.46G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac4cdde95e324b2ea6ab92c380279039",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64509c3931c84370960e2f8176e57df1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ff109ce990243beb0f88407e6d2c446",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/234 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "beea42fad99e499b851b831f9db42c0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1bfae8a68ac42f2857bfcee4e77186f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/454 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fef85aa562c7460ab1796a41dbaf31da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "659c3bf33eb74a84ae18c5985efd0fc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "chat_template.jinja: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = False # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
    "fourbit_models = [\n",
    "    \"unsloth/Meta-Llama-3.1-8B-bnb-4bit\",      # Llama-3.1 2x faster\n",
    "    \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",\n",
    "    \"unsloth/Meta-Llama-3.1-70B-bnb-4bit\",\n",
    "    \"unsloth/Meta-Llama-3.1-405B-bnb-4bit\",    # 4bit for 405b!\n",
    "    \"unsloth/Mistral-Small-Instruct-2409\",     # Mistral 22b 2x faster!\n",
    "    \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n",
    "    \"unsloth/Phi-3.5-mini-instruct\",           # Phi-3.5 2x faster!\n",
    "    \"unsloth/Phi-3-medium-4k-instruct\",\n",
    "    \"unsloth/gemma-2-9b-bnb-4bit\",\n",
    "    \"unsloth/gemma-2-27b-bnb-4bit\",            # Gemma 2x faster!\n",
    "\n",
    "    \"unsloth/Llama-3.2-1B-bnb-4bit\",           # NEW! Llama 3.2 models\n",
    "    \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\",\n",
    "    \"unsloth/Llama-3.2-3B-bnb-4bit\",\n",
    "    \"unsloth/Llama-3.2-3B-Instruct-bnb-4bit\",\n",
    "\n",
    "    \"unsloth/Llama-3.3-70B-Instruct-bnb-4bit\" # NEW! Llama 3.3 70B!\n",
    "] # More models at https://huggingface.co/unsloth\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Llama-3.2-3B-Instruct\", # or choose \"unsloth/Llama-3.2-1B-Instruct\"\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6lNgXncQMYY-",
    "outputId": "a3413ef1-6f40-4a5d-952f-990f587f9320"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.9.9 patched 28 layers with 28 QKV layers, 28 O layers and 28 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "d4a7168f13ed4069b9a900979241a7f9",
      "602f5648800f434b88be3655640a2a21",
      "e6d548ce0f544aa787ffb31779e83645",
      "58a5c48a312c4b9088da7ca1e1b0d78b",
      "4d36e03501864a148cf5497ceff116ed",
      "41c63aab32ef4bd68e1d82b6beae986b",
      "01d9e4f7f16c46ccb16494f2e7245974",
      "d923918a65bd48398399ae7b07bf72a2",
      "f0495c8319354ac4847b852c48066ace",
      "b7e688b96dde4feca12cc235e4a9e1dc",
      "07ce88b03eaa40bca620e28ca8cd9355"
     ]
    },
    "id": "LDkTrrY-L66M",
    "outputId": "67eef80f-cfe9-4ef8-ed1d-9f14d43d308f"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4a7168f13ed4069b9a900979241a7f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"json\", data_files=\"gnome_conversations.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "ebb2ae78e84747a78b826fbb18fb214b",
      "04521241520e40038a933beca7f05b76",
      "fd0f6bc4a0e3417abe12018f1845f124",
      "8f84544b00ed4c3996542a608e6eecae",
      "3fb3794f52534272b98965cc4f7e93e5",
      "1ff9eb7c2b514f48882a5fa74f68bace",
      "bf0122e5a2884d07a7a516e80005f920",
      "af4c6c81c6464ea8a5caef6813e62a8f",
      "a6ad798cae8f4a6f9350534f403a2eca",
      "ed1cfae0b90a4a8cb2e54af697bf5018",
      "82c2b03848ff47b784c0c02f9ead8501"
     ]
    },
    "id": "y1x5Bx0tL6-X",
    "outputId": "c5928efd-d912-467e-cba7-aa14fcf7fe1b"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebb2ae78e84747a78b826fbb18fb214b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/418 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template=\"llama-3.1\",\n",
    ")\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    convos = examples[\"messages\"]\n",
    "    texts = [tokenizer.apply_chat_template(convo, tokenize=False, add_generation_prompt=False) for convo in convos]\n",
    "    return { \"text\": texts }\n",
    "\n",
    "dataset = dataset.map(formatting_prompts_func, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Kcp3FUdeL7Ae",
    "outputId": "e296124e-5abc-4d21-b50b-30669012cba6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [[{'content': 'Do owls really stay awake all night?', 'role': 'user'}, {'content': 'Aye, they hoot and brood under moonlight while glaring at me like I stole their supper.', 'role': 'assistant'}], [{'content': 'Whatâ€™s your favorite kind of mushroom?', 'role': 'user'}, {'content': 'The red ones with white specksâ€”deadly, but they look like cheerful umbrellas for beetles.', 'role': 'assistant'}], [{'content': 'Can you tell me a bedtime story?', 'role': 'user'}, {'content': 'Once a squirrel buried an acorn so deep it sprouted upside down and grew roots into the sky. The end.', 'role': 'assistant'}], [{'content': 'Do you like rabbits?', 'role': 'user'}, {'content': 'Hmph, too twitchy and nosy. They dig holes where my moss bed should be.', 'role': 'assistant'}], [{'content': 'Whatâ€™s the fastest way through the forest?', 'role': 'user'}, {'content': 'Ask the deer, though they wonâ€™t wait for your stubby legs.', 'role': 'assistant'}]], 'text': ['<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nDo owls really stay awake all night?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nAye, they hoot and brood under moonlight while glaring at me like I stole their supper.<|eot_id|>', '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nWhatâ€™s your favorite kind of mushroom?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nThe red ones with white specksâ€”deadly, but they look like cheerful umbrellas for beetles.<|eot_id|>', '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nCan you tell me a bedtime story?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nOnce a squirrel buried an acorn so deep it sprouted upside down and grew roots into the sky. The end.<|eot_id|>', '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nDo you like rabbits?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nHmph, too twitchy and nosy. They dig holes where my moss bed should be.<|eot_id|>', '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nWhatâ€™s the fastest way through the forest?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nAsk the deer, though they wonâ€™t wait for your stubby legs.<|eot_id|>']}\n"
     ]
    }
   ],
   "source": [
    "train_dataset = dataset['train']\n",
    "print(train_dataset[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wO5PTmYzN448",
    "outputId": "2999e42f-a845-4e66-c5db-ee0be370d7d6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nDo owls really stay awake all night?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nAye, they hoot and brood under moonlight while glaring at me like I stole their supper.<|eot_id|>',\n",
       " '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nWhatâ€™s your favorite kind of mushroom?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nThe red ones with white specksâ€”deadly, but they look like cheerful umbrellas for beetles.<|eot_id|>']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[:2][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "JSIimC8xNjK7"
   },
   "outputs": [],
   "source": [
    "from trl import SFTConfig, SFTTrainer\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = train_dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer),\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = SFTConfig(\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 5,\n",
    "        # num_train_epochs = 1, # Set this for 1 full training run.\n",
    "        max_steps = 60,\n",
    "        learning_rate = 2e-4,\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "        report_to = \"none\", # Use this for WandB etc\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "e7c7075376724d3c8554e56a873bd778",
      "c5bfac9232c84fdab8ff65182949a095",
      "341b983e13904b5eab0240d7c1174612",
      "8c18d7fbfd504e70b0bd350faa8c113d",
      "d47f931f47ed4a98b252fc9c20f383fb",
      "d39aa5239f15469897c743b8f553ae46",
      "d3fb5af048614103b4815b49347db81e",
      "81b38f50c64a498582431cda30f02fb1",
      "f1f4b7ad63ac42db97ddb9476d0aa2f6",
      "f908b7a0d43c432c92450d83d3ad56d3",
      "7d1b751c6dc745d78b211af266c7baaa"
     ]
    },
    "id": "nKxdiki0NzB5",
    "outputId": "b574b5a8-35dd-4ea9-9e14-d1dd14d62208"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7c7075376724d3c8554e56a873bd778",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=2):   0%|          | 0/418 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from unsloth.chat_templates import train_on_responses_only\n",
    "trainer = train_on_responses_only(\n",
    "    trainer,\n",
    "    instruction_part = \"<|start_header_id|>user<|end_header_id|>\\n\\n\",\n",
    "    response_part = \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "hbGySr1MOwFr",
    "outputId": "d5cce380-f630-4f1c-c0d9-b6877161f6fc"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'<|begin_of_text|><|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nDo trees talk to each other?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nNot in words, fool. They gossip in roots and sigh in their leaves.<|eot_id|>'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(trainer.train_dataset[5][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "mufSn8zQNzKh",
    "outputId": "7cb0e107-52c9-4809-d067-ae30025dff09"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'                                           Not in words, fool. They gossip in roots and sigh in their leaves.<|eot_id|>'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "space = tokenizer(\" \", add_special_tokens = False).input_ids[0]\n",
    "tokenizer.decode([space if x == -100 else x for x in trainer.train_dataset[5][\"labels\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a9QnqaMtNzNd",
    "outputId": "9ed7aa79-217e-45a6-8ed4-9fefdb91c9f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU = Tesla T4. Max memory = 14.741 GB.\n",
      "3.07 GB of memory reserved.\n"
     ]
    }
   ],
   "source": [
    "# @title Show current memory stats\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "llFyDg3RO8w2",
    "outputId": "0e56e58d-98a4-4b36-ca04-b7890e8eaf3b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 418 | Num Epochs = 2 | Total steps = 60\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
      " \"-____-\"     Trainable parameters = 24,313,856 of 3,237,063,680 (0.75% trained)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [60/60 01:47, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.830500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.798100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.616000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3.211400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.671200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.957500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.653900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.499100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.344200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.221600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.212000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.128800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1.226700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1.081200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.168100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>1.257200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>1.114000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>1.065000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>1.151000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.100500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>1.073200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>1.071300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>1.189200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>1.122000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.993800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>1.108200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>1.001000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>1.086500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>1.008200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.024100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>1.061400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>1.002700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>1.021000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>1.031800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>1.053100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>1.097400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.980000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.924900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.960800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.940400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>1.114300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>1.055200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>1.060800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>1.059200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>1.033900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.925000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>1.197900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.982200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>1.002800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.939100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>1.040500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>0.961200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>0.903900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>0.849500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.864000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>0.786300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>0.833000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>0.745900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>0.710100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.876100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jrUX0SjeO8zX",
    "outputId": "9d8ce32b-e2bb-442c-e31b-3e8e3242df0d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nwhat should i wear this winter?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nI think fur or leaves. Thatâ€™s my winter plan, so far.<|eot_id|>']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"llama-3.1\",\n",
    ")\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"what should i wear this winter?\"},\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = True,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    return_tensors = \"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(input_ids = inputs, max_new_tokens = 64, use_cache = True,\n",
    "                         temperature = 1.5, min_p = 0.1)\n",
    "tokenizer.batch_decode(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3p-nvNeJO84p",
    "outputId": "7598a055-831b-4e92-ca0d-9ec9d16df17a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iâ€™ve tripped into music enough times to know itâ€™s silly.<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Hey Gnome, do you like dancing?\"},\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = True,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    return_tensors = \"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer, skip_prompt = True)\n",
    "_ = model.generate(input_ids = inputs, streamer = text_streamer, max_new_tokens = 128,\n",
    "                   use_cache = True, temperature = 1.5, min_p = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ONNf1UVJT8MA",
    "outputId": "15559290-41f4-4c1c-c7d5-2d9a55422f8b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop asking questions and laugh at its acorn stash. Thatâ€™ll ruin its day.<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"How to annoy a noisy squirrel ?\"},\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = True,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    return_tensors = \"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "text_streamer = TextStreamer(tokenizer, skip_prompt = True)\n",
    "_ = model.generate(input_ids = inputs, streamer = text_streamer, max_new_tokens = 128,\n",
    "                   use_cache = True, temperature = 1.5, min_p = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sG6GF8ThT8Wp"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17,
     "referenced_widgets": [
      "6fa0b6ad00d243cbac96ee2fde44de5b",
      "1db4b6bff3a94dbc9e55c4b37363afe7",
      "8599c9533f6b4a10bfb0b7ad50ae60a1",
      "7e41c1c34bcd4905836f5afe9c2a80ef",
      "46d0d7aace094c47ae213a7ec06054e2",
      "d871fec680ca4575992b4c211abb42d3",
      "f811d5589b6547a58f77a1e2d5cbf47f",
      "f4bb49bef4164cfcb4d35b6003c3836b",
      "4c33e89ed7ff4442adb956098eb0b092",
      "8bfff4bd47384d86af7014f6935776f9",
      "26c6cb54d73044bcb91fb53de71adb0f",
      "cf5e0f26a9344343b539e09d000792d0",
      "cb7a6d313b374f38b1394a06774c99aa",
      "6ec30f30281341b391a21204aa680625",
      "00359b26f8b64642808dbc317ed26360",
      "63240f10561f49ffb5a6129a663d8321",
      "6771177acf4044c78c0c3fc2158694cf",
      "bdba38e6290b40d4a19575fdfc38be98",
      "28f2ccf69cbc4842a6f4757963e30243",
      "7a04f8db30a645a0b3204831d8da80db"
     ]
    },
    "id": "-1cL5rQtO8-1",
    "outputId": "47af3ebd-c9c5-4087-bf7f-ae79bc3ee577"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fa0b6ad00d243cbac96ee2fde44de5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 162,
     "referenced_widgets": [
      "8f9c062ad5e6400a9a46feb4388ac039",
      "71ace0698bcd439eb455911e12046306",
      "9e1a602b26cd4427966aa21d0bfd281b",
      "aae2baa1e86b433d9d2d2e189aacbbd5",
      "c769f6966f9348c1a83faecfa9768da7",
      "11ad83b82f874acdb634f74b8e7d4127",
      "5d91ca1245a44204b3419c0ae5fabfa4",
      "5368f93793894c18b76a520e4bb75f18",
      "a78114d524db47c98a453a20e85d8bfa",
      "4755a36211f34f5dad11cf9abaf63431",
      "39906bdee2b54c85af731a4c0960108b",
      "bbb7b2405f0a4e639bbe25959c8fbc70",
      "f0a72b16b58c4081aff03f4d1aa73f0b",
      "d864aa7cf37d474c9f86b79493fd26e1",
      "0cff43c48f6846d48afb9a8a35a740d7",
      "a24108f56aa94a08bba8e63186640247",
      "7b4e308e2d5b4249bfed91f059db1f87",
      "961c5c4532e04cd2ae4ceacb5dae4c86",
      "57ae77ae6f0d42058d6efae330eccebb",
      "ce349e7e06a04808b6eb572549c87d58",
      "246edcad47464b8fb3ede006927a32a4",
      "593fa39192684a69b54aa518e360d0a6",
      "4dea5b6a8e824c0584b2485c590bbd73",
      "7a543051e365479f846ee1babac4716a",
      "b2dc19fffb3c40cc84edb3b2e3d47b5a",
      "0147f6407df346ae895abd9e948d94b7",
      "bf7770de3f8c47c5ab04dc22d34d33f2",
      "fceb5812c72744088ce5340ead701367",
      "718d182753c74daf88a6b32ef23d0151",
      "45bcf6ec4576452ea5759eecf9fe78d0",
      "bf71c7c9c4094a4bb69fad157ff692de",
      "6b9b0ab2f40249d296bfd5958410aafe",
      "5bae8acda06c49c1ae1af725117d8008",
      "f4d205ad75fb4814b48638dd2a0ebc0d",
      "47a50dd9fd86401a818ce42d7a917938",
      "38cfe1b0e9fb4daf836c57762be73f19",
      "ae71a805655f40b7a50cf335187e3dab",
      "0362c94a381246b697f57fc1726c3b78",
      "f17d01f30c8c459ebcff368d8da9293e",
      "66207786b9934024aee5d3c0db4c9197",
      "6d7c7892f66e429cbab514782433b397",
      "623a1204d4ec4a57b48a3b29dcf0a0d5",
      "5bd3bad67422465fb3da496dcec88c66",
      "4278ad3c0e9c4de3bc96e2b5227d4688"
     ]
    },
    "id": "At2HbwvJUflt",
    "outputId": "19c59ab4-e23b-4d3d-a83d-e23831535ee6"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f9c062ad5e6400a9a46feb4388ac039",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/592 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbb7b2405f0a4e639bbe25959c8fbc70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0)      : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dea5b6a8e824c0584b2485c590bbd73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload               : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4d205ad75fb4814b48638dd2a0ebc0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  ...adapter_model.safetensors:   0%|          | 45.8kB / 97.3MB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to https://huggingface.co/anastasiia-strashnova/gnome-chatbot-lora\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import HfApi\n",
    "\n",
    "repo_name = \"anastasiia-strashnova/gnome-chatbot-lora\"\n",
    "model.push_to_hub(repo_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MMDEb09QUfol"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
